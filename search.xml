<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Parsing Logs with Logstash]]></title>
    <url>%2F2017%2F06%2F24%2FParsing%20Logs%20with%20Logstash%2F</url>
    <content type="text"><![CDATA[步骤 deb: 12# curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-5.4.2-amd64.deb# sudo dpkg -i filebeat-5.4.2-amd64.deb 安装后可以看到如下 Parsing Logs with Logstashfilebeat config 在安装完成之后你需要配置filebeat，通过修改filebeat.yml，以nginx的日志为例 12# cd /etc/filebeat# vi filebeat.yml filebeat.yml 12345678910111213141516filebeat.prospectors:# Each - is a prospector. Most options can be set at the prospector level, so# you can use different prospectors for various configurations.# Below are the prospector specific configurations.- input_type: log # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/nginx/*.log#----------------------------- Logstash output --------------------------------output.logstash: # The Logstash hosts hosts: ["localhost:5044"] 在数据源的机器上执行如下命令:12# cd /usr/share/filebeat/bin/# ./filebeat -e -c /etc/filebeat/filebeat.yml -d "publish" logstash config 创建一个logstash的配置文件 12# cd /etc/logstash/conf.d# vi first-pipeline.conf first-pipeline.conf 12345678910111213input &#123; beats &#123; port =&gt; "5044" &#125;&#125;# The filter part of this file is commented out to indicate that it is# optional.# filter &#123;## &#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125;&#125; 确认logstash配置文件是否写错 12# cd /usr/share/logstash/# bin/logstash --path.settings /etc/logstash -f /etc/logstash/conf.d/first-pipeline.conf --config.test_and_exit 获取ok ( The –config.test_and_exit option parses your configuration file and reports any errors.) 12Sending Logstash&apos;s logs to /var/log/logstash which is now configured via log4j2.propertiesConfiguration OK 运行logstash 1# bin/logstash --path.settings /etc/logstash -f /etc/logstash/conf.d/first-pipeline.conf --config.reload.automatic 在filebeat开启的时候可以获得类似于如下的信息：123456789101112131415161718&#123; &quot;@timestamp&quot; =&gt; 2017-06-22T14:58:32.169Z, &quot;offset&quot; =&gt; 13300, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;input_type&quot; =&gt; &quot;log&quot;, &quot;beat&quot; =&gt; &#123; &quot;hostname&quot; =&gt; &quot;iZm5e7jlki70utmw22zj76Z&quot;, &quot;name&quot; =&gt; &quot;iZm5e7jlki70utmw22zj76Z&quot;, &quot;version&quot; =&gt; &quot;5.4.2&quot; &#125;, &quot;host&quot; =&gt; &quot;iZm5e7jlki70utmw22zj76Z&quot;, &quot;source&quot; =&gt; &quot;/var/log/nginx/access.log&quot;, &quot;message&quot; =&gt; &quot;115.61.84.162 - - [22/Jun/2017:22:06:06 +0800] \&quot;GET http://open.163.com/ HTTP/1.1\&quot; 200 612 \&quot;http://open.163.com/\&quot; \&quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\&quot;&quot;, &quot;type&quot; =&gt; &quot;log&quot;, &quot;tags&quot; =&gt; [ [0] &quot;beats_input_codec_plain_applied&quot; ]&#125; 修改first-pipeline.conf，使用grok 12345678910111213input &#123; beats &#123; port =&gt; &quot;5044&quot; &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;COMBINEDAPACHELOG&#125;&quot;&#125; &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125;&#125; 删除日志记录的节点，我们可以重头读取日志 12# rm /usr/share/filebeat/bin/data/registry# bin/logstash --path.settings /etc/logstash -f /etc/logstash/conf.d/first-pipeline.conf --config.reload.automatic 可以看到我们获取到了的日志改变了，获取了更加详细的日志 启用goip 12345678910111213141516input &#123; beats &#123; port =&gt; &quot;5044&quot; &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;COMBINEDAPACHELOG&#125;&quot;&#125; &#125; geoip &#123; source =&gt; &quot;clientip&quot; &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125;&#125; 同样删除registry，重启程序，可以看到信息又更新了 indexing data into elasticsearch 将first-pipeline.conf 再度修改，将output指向elasticsearch 123456789101112131415161718192021input &#123; beats &#123; port =&gt; &quot;5044&quot; &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;COMBINEDAPACHELOG&#125;&quot;&#125; &#125; geoip &#123; source =&gt; &quot;clientip&quot; &#125;&#125;#output &#123;# stdout &#123; codec =&gt; rubydebug &#125;#&#125;output &#123; elasticsearch &#123; hosts =&gt; [ &quot;localhost:9200&quot; ] &#125;&#125; 同样删除registry，重启程序，然后再启动elasticsearch 123# systemctl start elasticsearch.service# curl -XGET &apos;http://localhost:9200/logstash-2017.06.23/_search?pretty&amp;q=response=200&apos;# curl -XGET &apos;http://localhost:9200/logstash-2017.06.23/_search?pretty&amp;q=geoip.city_name=Zhengzhou&apos; 命令中logstash-DATE，替换 DATE 变成正确的时间, 格式如下 YYYY.MM.DD，通过请求，我们可以获得类似如下的数据 完整版 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&#123; &quot;took&quot; : 11, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : 2, &quot;max_score&quot; : 2.1282318, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;logstash-2017.06.23&quot;, &quot;_type&quot; : &quot;log&quot;, &quot;_id&quot; : &quot;AVzTKaFXQUT1Lre_3sa9&quot;, &quot;_score&quot; : 2.1282318, &quot;_source&quot; : &#123; &quot;request&quot; : &quot;http://www.dajie.com/&quot;, &quot;agent&quot; : &quot;\&quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\&quot;&quot;, &quot;geoip&quot; : &#123; &quot;city_name&quot; : &quot;Zhengzhou&quot;, &quot;timezone&quot; : &quot;Asia/Shanghai&quot;, &quot;ip&quot; : &quot;115.61.84.162&quot;, &quot;latitude&quot; : 34.6836, &quot;country_name&quot; : &quot;China&quot;, &quot;country_code2&quot; : &quot;CN&quot;, &quot;continent_code&quot; : &quot;AS&quot;, &quot;country_code3&quot; : &quot;CN&quot;, &quot;region_name&quot; : &quot;Henan&quot;, &quot;location&quot; : &#123; &quot;lon&quot; : 113.5325, &quot;lat&quot; : 34.6836 &#125;, &quot;region_code&quot; : &quot;41&quot;, &quot;longitude&quot; : 113.5325 &#125;, &quot;offset&quot; : 182, &quot;auth&quot; : &quot;-&quot;, &quot;ident&quot; : &quot;-&quot;, &quot;input_type&quot; : &quot;log&quot;, &quot;verb&quot; : &quot;GET&quot;, &quot;source&quot; : &quot;/var/log/nginx/access.log&quot;, &quot;message&quot; : &quot;115.61.84.162 - - [23/Jun/2017:06:50:10 +0800] \&quot;GET http://www.dajie.com/ HTTP/1.1\&quot; 200 612 \&quot;http://www.dajie.com/\&quot; \&quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\&quot;&quot;, &quot;type&quot; : &quot;log&quot;, &quot;tags&quot; : [ &quot;beats_input_codec_plain_applied&quot; ], &quot;referrer&quot; : &quot;\&quot;http://www.dajie.com/\&quot;&quot;, &quot;@timestamp&quot; : &quot;2017-06-23T04:15:32.219Z&quot;, &quot;response&quot; : &quot;200&quot;, &quot;bytes&quot; : &quot;612&quot;, &quot;clientip&quot; : &quot;115.61.84.162&quot;, &quot;@version&quot; : &quot;1&quot;, &quot;beat&quot; : &#123; &quot;hostname&quot; : &quot;iZm5e7jlki70utmw22zj76Z&quot;, &quot;name&quot; : &quot;iZm5e7jlki70utmw22zj76Z&quot;, &quot;version&quot; : &quot;5.4.2&quot; &#125;, &quot;host&quot; : &quot;iZm5e7jlki70utmw22zj76Z&quot;, &quot;httpversion&quot; : &quot;1.1&quot;, &quot;timestamp&quot; : &quot;23/Jun/2017:06:50:10 +0800&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;logstash-2017.06.23&quot;, &quot;_type&quot; : &quot;log&quot;, &quot;_id&quot; : &quot;AVzTOcnEQUT1Lre_3sbu&quot;, &quot;_score&quot; : 2.1282318, &quot;_source&quot; : &#123; &quot;request&quot; : &quot;http://www.dajie.com/&quot;, &quot;agent&quot; : &quot;\&quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\&quot;&quot;, &quot;geoip&quot; : &#123; &quot;city_name&quot; : &quot;Zhengzhou&quot;, &quot;timezone&quot; : &quot;Asia/Shanghai&quot;, &quot;ip&quot; : &quot;115.61.84.162&quot;, &quot;latitude&quot; : 34.6836, &quot;country_name&quot; : &quot;China&quot;, &quot;country_code2&quot; : &quot;CN&quot;, &quot;continent_code&quot; : &quot;AS&quot;, &quot;country_code3&quot; : &quot;CN&quot;, &quot;region_name&quot; : &quot;Henan&quot;, &quot;location&quot; : &#123; &quot;lon&quot; : 113.5325, &quot;lat&quot; : 34.6836 &#125;, &quot;region_code&quot; : &quot;41&quot;, &quot;longitude&quot; : 113.5325 &#125;, &quot;offset&quot; : 182, &quot;auth&quot; : &quot;-&quot;, &quot;ident&quot; : &quot;-&quot;, &quot;input_type&quot; : &quot;log&quot;, &quot;verb&quot; : &quot;GET&quot;, &quot;source&quot; : &quot;/var/log/nginx/access.log&quot;, &quot;message&quot; : &quot;115.61.84.162 - - [23/Jun/2017:06:50:10 +0800] \&quot;GET http://www.dajie.com/ HTTP/1.1\&quot; 200 612 \&quot;http://www.dajie.com/\&quot; \&quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\&quot;&quot;, &quot;type&quot; : &quot;log&quot;, &quot;tags&quot; : [ &quot;beats_input_codec_plain_applied&quot; ], &quot;referrer&quot; : &quot;\&quot;http://www.dajie.com/\&quot;&quot;, &quot;@timestamp&quot; : &quot;2017-06-23T04:33:13.588Z&quot;, &quot;response&quot; : &quot;200&quot;, &quot;bytes&quot; : &quot;612&quot;, &quot;clientip&quot; : &quot;115.61.84.162&quot;, &quot;@version&quot; : &quot;1&quot;, &quot;beat&quot; : &#123; &quot;hostname&quot; : &quot;iZm5e7jlki70utmw22zj76Z&quot;, &quot;name&quot; : &quot;iZm5e7jlki70utmw22zj76Z&quot;, &quot;version&quot; : &quot;5.4.2&quot; &#125;, &quot;host&quot; : &quot;iZm5e7jlki70utmw22zj76Z&quot;, &quot;httpversion&quot; : &quot;1.1&quot;, &quot;timestamp&quot; : &quot;23/Jun/2017:06:50:10 +0800&quot; &#125; &#125; ] &#125;&#125; 去web上查看结果 参考资料： 官方文档： filebeat 官方文档： logstash]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install Kibana with Debian Package]]></title>
    <url>%2F2017%2F06%2F22%2FInstall%20Kibana%20with%20Debian%20Package%2F</url>
    <content type="text"><![CDATA[Kibana install步骤 下载并安装公共签名密钥 1# wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - deb-repo 你可能需要在Debian安装apt-transport-https: 1# apt-get install apt-transport-https 将 repository 保存到 /etc/apt/sources.list.d/elastic-5.x.list 1# echo "deb https://artifacts.elastic.co/packages/5.x/apt stable main" | tee -a /etc/apt/sources.list.d/elastic-5.x.list 你可以安装 Debian package Logstash,命令如下: 1# apt-get update &amp;&amp; apt-get install kibana deb-package 64 123# wget https://artifacts.elastic.co/downloads/kibana/kibana-5.4.2-amd64.deb# sha1sum kibana-5.4.2-amd64.deb# dpkg -i kibana-5.4.2-amd64.deb 32 123# wget https://artifacts.elastic.co/downloads/kibana/kibana-5.4.2-i386.deb# sha1sum kibana-5.4.2-i386.deb# dpkg -i kibana-5.4.2-i386.deb 通过输入命令来查询logstash安装在哪里了 1# whereis kibana 获得输出结果 1kibana: /etc/kibana /usr/share/kibana 配置kibana随系统启动，运行下面的命令: 12# /bin/systemctl daemon-reload# /bin/systemctl enable kibana.service kibanah可以按照如下方式启动和停止: 12# systemctl start kibana.service# systemctl stop kibana.service localhost:5601 or http://YOURDOMAIN.com:5601 参考资料 kibana connect-to-elasticsearch]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install Elasticsearch with Debian Package]]></title>
    <url>%2F2017%2F06%2F21%2FElasticsearch_install%2F</url>
    <content type="text"><![CDATA[Install Elasticsearch with Debian Package for Debian, Ubuntu, and other Debian-based systems 需要java环境,请自行准备 官方参考资料：https://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html#deb-key 其他版本系统参考资料：https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html 步骤 下载并安装公共签名密钥 1# wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - 你可能需要在Debian安装apt-transport-https: 1# apt-get install apt-transport-https 将 repository 保存到 /etc/apt/sources.list.d/elastic-5.x.list 1# echo "deb https://artifacts.elastic.co/packages/5.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-5.x.list 你可以安装 Debian package Elasticsearch,命令如下: 1# apt-get update &amp;&amp; sudo apt-get install elasticsearch Elasticsearch v5.4.2 Debian 软件包可以从网站上下载并安装: 123# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.4.2.deb# sha1sum elasticsearch-5.4.2.deb# sudo dpkg -i elasticsearch-5.4.2.deb 配置Elasticsearch随系统启动，运行下面的命令: 12# /bin/systemctl daemon-reload# /bin/systemctl enable elasticsearch.service Elasticsearch可以按照如下方式启动和停止: 12# systemctl start elasticsearch.service# systemctl stop elasticsearch.service 当系统启用了日志记录，日志记录的信息都可以用journalctl命令: To tail the journal: 1# journalctl -f To list journal entries for the elasticsearch service: 1# journalctl --unit elasticsearch To list journal entries for the elasticsearch service starting from a given time: 1# journalctl --unit elasticsearch --since "2016-10-30 18:17:16" 我的机器是阿里云1c 1G的,启动的时候就报出如下错误 123# Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000085330000, 2060255232, 0) failed; error='Cannot alloc# There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 2060255232 bytes for committing reserved memory. 根据机器配置不同可以修改相对应的参数 1# vi /etc/elasticsearch/jvm.options 将 -Xms 和 -Xmx 参数修改一下就可以正常启动了 12-Xms256m-Xmx256m 默认参数是运行在9200端口下的,可以通过如下命令确认: 1# curl -XGET 'localhost:9200/?pretty' 返回结果如下 12345678910111213&#123; "name" : "0NcYzNP", "cluster_name" : "elasticsearch", "cluster_uuid" : "-QmXkqg6T0uVrUAbNZhggA", "version" : &#123; "number" : "5.4.2", "build_hash" : "929b078", "build_date" : "2017-06-15T02:29:28.122Z", "build_snapshot" : false, "lucene_version" : "6.5.1" &#125;, "tagline" : "You Know, for Search"&#125; 配置Elasticsearch Elasticsearch 的默认加载配置路径： /etc/elasticsearch/elasticsearch.yml . 配置的详细解释可以参考官方文档：elasticsearch Debian package 也有一个配置文件 (/etc/default/elasticsearch), 允许你配置如下参数: 参数 注释 ES_USER The user to run as, defaults to elasticsearch ES_GROUP The group to run as, defaults to elasticsearch JAVA_HOME Set a custom Java path to be used MAX_OPEN_FILES Maximum number of open files, defaults to 65536 MAX_LOCKED_MEMORY Maximum locked memory size. Set to unlimited if you use the bootstrap.memory_lock option in elasticsearch.yml MAX_MAP_COUNT Maximum number of memory map areas a process may have. If you use mmapfs as index store type, make sure this is set to a high value. For more information, check the linux kernel documentation about max_map_count. This is set via sysctl before starting elasticsearch. Defaults to 262144 LOG_DIR Log directory, defaults to /var/log/elasticsearch DATA_DIR Data directory, defaults to /var/lib/elasticsearch CONF_DIR Configuration file directory (which needs to include elasticsearch.yml and log4j2.properties files), defaults to /etc/elasticsearch ES_JAVA_OPTS Any additional JVM system properties you may want to apply RESTART_ON_UPGRADE Configure restart on package upgrade, defaults to false. This means you will have to restart your elasticsearch instance after installing a package manually. The reason for this is to ensure, that upgrades in a cluster do not result in a continuous shard reallocation resulting in high network traffic and reducing the response times of your cluster The Debian package places config files, logs, and the data directory in the appropriate locations for a Debian-based systemhttps://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html#deb-layout Type Description Default Location Setting home Elasticsearch home directory or $ES_HOME /usr/share/elasticsearch bin Binary scripts including elasticsearch to start a node and elasticsearch-plugin to install plugins /usr/share/elasticsearch/bin conf Configuration files including elasticsearch.yml /etc/elasticsearch path.conf conf Environment variables including heap size, file descriptors /etc/default/elasticsearch data The location of the data files of each index / shard allocated on the node. Can hold multiple locations. /var/lib/elasticsearch path.data logs Log files location. /var/log/elasticsearch path.logs plugins Plugin files location. Each plugin will be contained in a subdirectory. /usr/share/elasticsearch/plugins repo Shared file system repository locations. Can hold multiple locations. A file system repository can be placed in to any subdirectory of any directory specified here. Not configured path.repo script Location of script files. /etc/elasticsearch/scripts path.scripts]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于itchat实现微信发送信息]]></title>
    <url>%2F2017%2F06%2F21%2Fitchat_wechat%2F</url>
    <content type="text"><![CDATA[前景提要 官方介绍 ：http://itchat.readthedocs.io/zh/latest/ python 环境支持 2.7 , 3.5 本人环境使用的是python 3.5 版本的，其他版本的请适当修改代码 创建一个登录脚本，维持稳定在线 12345#!/usr/bin/python3# -*- coding: utf-8 -*-import itchatitchat.auto_login(hotReload=True, enableCmdQR=True)itchat.run() 12# chmod +x login.py# ./login.py 通过手机扫描二维码即可登录 退出程序ctrl +c 1# nohup python3.5 login.py &amp; 将程序挂起，就不需要继续扫码了，然后我们可以在这里找到一个itchat.pkl的文件 这个就是我们需要调用的东西了，靠他可以发送微信消息了 1# vi send_msg.py 12345678910#!/usr/bin/python3# -*- coding: utf-8 -*-import itchatmsg = 'hello'# 此处为itchat.pkl文件绝对路径path_pkl = r'/usr/lib/zabbix/alertscripts/itchat.pkl'itchat.auto_login(hotReload=True, statusStorageDir=path_pkl)# 我是一个新创建的微信号，只拉了一个讨论组，所以就默认发送第一个讨论组group = itchat.get_chatrooms(update=True)itchat.send(msg=msg, toUserName=group[0]["UserName"]) 12# chmod +x send_msg.py# ./send_msg.py 然后就可以在微信讨论组里看到了通过脚本控制发送的消息]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logstash install]]></title>
    <url>%2F2017%2F06%2F20%2FLogstash_install%2F</url>
    <content type="text"><![CDATA[Logstash install Logstash 需要java环境 java 9 不支持，请自行安装 输入命令可以查询java版本 1# java -version 可以获得如下信息 123# java version "1.8.0_131"# Java(TM) SE Runtime Environment (build 1.8.0_131-b11)# Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode) 步骤 下载并安装公共签名密钥 1# wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - 你可能需要在Debian安装apt-transport-https: 1# apt-get install apt-transport-https 将 repository 保存到 /etc/apt/sources.list.d/elastic-5.x.list 1# echo "deb https://artifacts.elastic.co/packages/5.x/apt stable main" | tee -a /etc/apt/sources.list.d/elastic-5.x.list 你可以安装 Debian package Logstash,命令如下: 1# apt-get update &amp;&amp; apt-get install logstash 通过输入命令来查询logstash安装在哪里了 1# whereis logstash 获得输出结果 1logstash: /etc/logstash /usr/share/logstash 测试logstash 12# cd /usr/share/logstash# bin/logstash -e 'input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;' 具体运行结果如下 出现警告信息：是没有找到配置路径，把命令改一下 1./bin/logstash --path.settings /etc/logstash -e 'input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;' 具体运行结果如下 参考资料 官网文档地址 ：logstash]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vps最近测评]]></title>
    <url>%2F2016%2F08%2F17%2Fvps%E6%9C%80%E8%BF%91%E6%B5%8B%E8%AF%84%2F</url>
    <content type="text"><![CDATA[近日虚拟主机实测随着云技术的越来越成熟，很多朋友的建站需求不仅仅满足使用虚拟主机，打算使用vps，虚拟主机的性能，带宽等多方面是需要考虑的。最近苦寻高性价比的vps，来介绍一下我所使用的vps： 第一站，阿里云阿里云作为国内云平台数一数二的，首先我想到的就是阿里云 在阿里云官网购买了最低配置的ecs云服务器 1C 1G 1M的 月付61.5RMB 测试阿里云的io 网络检测：站长之家ping检测 就目前使用状况而言，阿里云的机器性能质量不尽如人意，网络状况值得称赞，稳定，而且阿里云有学生优惠活动，十分优惠，提供免费的备案服务，只要在阿里云购买服务器，即可享受，国内还是首推阿里云 第二站，ConoHa第二家，来到了号称全网ssd云盘的Conoha，对于这家外国的vps厂商了解不多，但是，他支持支付宝付款！！！ 同样选择最低配置 但这个是2C 1G的vps，ssd云盘！！！ 购买了一台，已经启动了，月付900日元，月付约60RMB，每月的话费大致和阿里云相当 内存 cpu信息 下载测速，效果看起来不错，是不限带宽，不限流量的，不过有网友评论这家大肆使用流量的话会被封号，不知道是不是真的 来让我们看看io，是否是ssd，效果拔群 ping检测，博主本人家有电信，移动两条线路，找朋友测试联通线路，经过测试，坐标杭州电信ping 200+移动ping 70+联通ping 80+ 博主电信线路tarcert了一下ip，路线显示：杭州-&gt;北京-&gt;上海-&gt;美国-&gt;日本 电信的线路连接日本还不如买一个美国的vps连接呢，希望能找到一家CN2优化的线路 conoha就线路而言，感觉对电信并不怎么友好 网络检测：站长之家ping检测 #未完待续…]]></content>
      <categories>
        <category>vps测评</category>
      </categories>
      <tags>
        <tag>vps</tag>
      </tags>
  </entry>
</search>